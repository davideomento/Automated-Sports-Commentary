import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login
from shutil import copytree
from transformers.utils import cached_file



# Directory dove vuoi salvare il modello su Drive
drive_model_path = "/content/drive/MyDrive/mistral_model"

# Nome del modello Hugging Face
model_name = "mistralai/Mistral-7B-Instruct-v0.1"

# Inserisci il tuo token
hf_token = "HF_TOKEN"  # Inserisci il tuo token HF qui
login(hf_token)

# Se il modello √® gi√† presente su Drive, lo carico da l√¨
if os.path.exists(drive_model_path):
    print(f"‚úÖ Modello gi√† presente in {drive_model_path}, lo carico da l√¨.")
    tokenizer = AutoTokenizer.from_pretrained(drive_model_path)
    model = AutoModelForCausalLM.from_pretrained(drive_model_path, device_map="auto")
else:
    print("‚¨áÔ∏è Modello non trovato su Drive, scarico da Hugging Face...")

    # Scarica tokenizer e modello
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", token=hf_token)

    # Trova percorso locale della cache HF
    print("üìÅ Copio i file dalla cache locale al Drive...")
    cache_path = os.path.dirname(
        cached_file(model_name, "model.safetensors.index.json", token=hf_token)
    )
    
    # Copia il contenuto su Drive
    copytree(cache_path, drive_model_path, dirs_exist_ok=True)
    tokenizer.save_pretrained(drive_model_path)

